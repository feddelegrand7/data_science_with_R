<style>

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 2em;
}

.reveal h3 {
  font-size: 1.4em;
}

.reveal h4 {
  font-size: 1.2em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.smallcode pre code {
  font-size: 1em;
}

.reveal .smalltext {
  font-size: 0.8em;
}

</style>


Data Science with R
========================================================
author: Sigrid Keydana, Trivadis
date: 
autosize: true

Agenda
========================================================
class:smalltext

&nbsp;

### Part 1: Concepts, basics, methods

1. Statistics, data mining, predictive analytics, data science, machine learning, deep learning... what???
2. Introduction to statistical modeling: supervised vs. unsupervised, regression vs. classification, prediction vs. inference 
3. R for data science
4. Exploratory data analysis and visualization (modelr & hadley case studies???)
5. Important concepts in statistical modeling: overfitting, bias/variance and model selection
6. Some statistical background
7. Introduction to supervised learning (1): Linear regression
8. Introduction to supervised learning (2): Logistic regression
9. Algorithms: regression and classification trees, support vector machines, k-nearest-neighbors
10. Ensemble methods
11. Model selection revisited: Cross validation, subset selection and regularization (labs!!!)
11. Introduction to unsupervised learning: clustering, principal components analysis

### Part 2: Case study: from data to models

fraud  detection


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Introduction to statistical modeling
</h1>


Example: Advertising
========================================================

&nbsp;

Does advertising help sales? How much? What about the different channels - should we advertise on TV? Radio? Newspapers?

<figure>
<img src="2.1.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Example: Income
========================================================

&nbsp;

Does income increase with education? How much?

<figure>
<img src="2.2.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>





Example: Clustering gene expression measurements for cancer cells
========================================================

&nbsp;

<figure>
<img src="1.4.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>





Supervised and unsupervised learning
========================================================

&nbsp;

#### Supervised learning

- goal: learn a function from input to output, &nbsp;&nbsp;&nbsp; $y = f(x)$
- workflow: train on _training set_ (and possibly, validation set), test performance on _test set_
- for both training and test set, $x$ and $y$ are given (ground truth)

#### Unsupervised learning

- goal: find patterns (groups, clusters...) in a dataset
- $x$ is given, but $y$ is not


Regression and classification
========================================================

&nbsp;

#### Regression

- output variable is quantitative
    - predict weight from height
    - predict house price from habitable surface and location
    - predict income from parents' income
    
#### Classification

- output variable is qualitative
    - predict credit card default
    - predict disease yes/no
    - predict intervention success


Prediction vs. inference
========================================================

&nbsp;

What's more important: 

- predict a new $\hat y$ as accurately as possible?
- model the underlying relationship between $x$ and $y$ (or even: _explain_ $y$ by $x$?)



Parametric vs. nonparametric methods
========================================================

&nbsp;

#### Parametric

- assume a specific form for $f$ in $y = f(x)$
- e.g., linear regression: $f(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p$
- then, estimate the $\beta_0, \beta_1 ... \beta_p$
- functions with few parameters may be too unflexible to model the true $f$
- functions with many parameters may _overfit_ on the training data (more on overfitting later)

#### Non-parametric

- no explicit assumptions about the underlying $f$
- more flexible than parametric methods, but also more prone to _overfitting_


Prediction accuracy vs. model interpretability
========================================================

&nbsp;

By method

<figure>
<img src="2.7.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



========================================================

========================================================

========================================================

========================================================

========================================================




========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
R for data science
</h1>


From S to R
========================================================
class:smalltext

&nbsp;

#### Before there was R, there was S

- Statistical computing language developed at Bell Laboratories by John Chambers et al. (1976)
- Major versions S3 and S4
- Mission statement: _turn ideas into software, quickly and faithfully_

#### S-PLUS

- Commercial version of S with eventful history (as of today, integrated into Tibco Spotfire)

#### R

- Developed by Ross Ithaka and Robert Gentleman at University of Auckland (1995-2000)
- Syntax and object system from S, scoping rules and environment model from Scheme 
- Released under GNU Public License (GPL)


The R ecosystem
========================================================

&nbsp;

- CRAN (Comprehensive R Archive Network): repository with more than 10,000 user-contributed packages
- RStudio: most popular R IDE (can do “everything” in RStudio)
- If you can just choose one person to stand for it all: Hadley Wickham (Chief Scientist at RStudio), creator of the _tidyverse_
- Books: many. We will partly use examples from _R for Data Science_ by the above Hadley Wickham 


Why R (for data science)
========================================================

&nbsp;

- basically every statistical or machine learning algorithm is implemented in R
- even if you have to implement a new algorithm yourself, you can build upon a strong fundament (optimization, matrix computations ...)
- elegant, concise syntax, esp. when using packages from the _tidyverse_
- quickly try out things and experiment on the (RStudio) Console
- beautiful graphics
- (personal opinion) programming R is fun!


Getting started with R (1)
========================================================
class:smallcode

&nbsp;

#### Basic data types and assignment

```{r, results="hide" }
2      # a number
class(2)
2.0    # the same number
class(2.0)

"two"  # a string (character vector)
'two'  # can also use single quotes

TRUE == FALSE # boolean datatype, called logical in R

x <- 1 # assignment
x
y = 2 # can also use =

```


Getting started with R (2)
========================================================
class:smallcode

&nbsp;

#### Data structure

```{r, results="hide" }
z <- c(1,2,3) # a vector of doubles
zz <- c("a", "b", "c") # a vector of character strings

# actually, all objects are non-scalars, there is no real difference
length(c(1,2,3))


zzz <- factor(zz) # a factor (for categorical data)

```




========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Important concepts in statistical modeling
</h1>


Training error, test error, and overfitting
========================================================

&nbsp;

A model that minimizes training error but incurs high test error is said to _overfit_ the training data.

While good performance on the training set is nice, the thing we really care about is _low error on the test set_ (low _generalization error_).

<figure>
<img src="2.9.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

There is no such thing as overall optimal complexity (1)
========================================================

&nbsp;

Three unequal-complexity models with training and test errors

_moderately nonlinear data_

<figure>
<img src="2.9.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

There is no such thing as overall optimal complexity (2)
========================================================

&nbsp;

Three unequal-complexity models with training and test errors

_highly linear data_

<figure>
<img src="2.10.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

There is no such thing as overall optimal complexity (3)
========================================================

&nbsp;

Three unequal-complexity models with training and test errors

_very nonlinear data_

<figure>
<img src="2.11.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

How to prevent overfitting
========================================================

&nbsp;

- use cross-validation to determine optimal model complexity 
- penalize more complex models (regularization)

We'll cover cross validation below and see regularization later.


Bias-variance tradeoff (1)
========================================================

&nbsp;

The expected test error decomposes into 3 separate components:

- the _variance_ of the estimator due to sample variation
- the _bias_ due to approximating the true relationship with a simplification
- the noise variance (irreducible error)

&nbsp;

$E(y_0 - \hat f(x_0))^2 = var(\hat f(x_0)) + (bias(\hat f(x_0)))^2 + var(\epsilon)$


Bias-variance tradeoff (2)
========================================================

&nbsp;

<figure>
<img src="2.12.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Test MSE, with its three components variance, bias and irreducible error, vs. _flexibility_, for the three scenarios above. 
Dashed line is irreducible error.


Model selection
========================================================

&nbsp;

How do we select the best model?

The most common approach is _cross-validation_.

(Another is imposing priors / using regularization).


How cross-validation works
========================================================

&nbsp;

Example: 5-fold cross-validation.

<figure>
<img src="5.5.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Supervised learning: linear regression
</h1>


What is the effect of advertising on sales?
========================================================

&nbsp;

In simple (= single-predictor) linear regression we fit a line to the data:

$\hat y = \beta_0 + \beta_1 x$

<figure>
<img src="2.1.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


How do we estimate the coefficients?
========================================================

&nbsp;

The most common approach by far is _least squares_ where we minimize the _residual sum of squares_

$(y_1 - \hat \beta_0 - \hat \beta_1 x_1)^2 + (y_2 - \hat \beta_0 - \hat \beta_1 x_2)^2 + ... + (y_n - \hat \beta_0 - \hat \beta_1 x_n)^2$

<figure>
<img src="3.1.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Which gives the estimates
========================================================

&nbsp;


$\hat \beta_0 = \bar y - \hat \beta_1 \bar x$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

$\hat \beta_1 = cor(y, x) \frac{sd(y)}{sd(x)}$

&nbsp;

That means, the slope of the line

- gets steeper when $x$ and $y$ are more correlated
- gets steeper when there's more variation in the $y$'s
- gets shallower when there's more variation in the $x$'s

How good is the model fit?
========================================================

&nbsp;

Model performance in the one-predictor case is usually assessed by calculating the proportion of variance explained:

$R^2 = \frac{total sum of squares - residual sum of squares}{total sum of squares} = 1 - \frac{residual sum of squares}{total sum of squares}$

This $R^2$ is the square of the correlation coefficient $R$.


Multiple predictors
========================================================

&nbsp;

With n predictors, we don't fit a line any more, but an n-dimensional hyperplane.

This is an example of the regression of a hypothetical $y$ on two hypothetical predictors $x_1$ and $x_2$.

<figure>
<img src="3.4.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


What it means
========================================================

&nbsp;

In multiple regression, a predictor is tested for its impact _holding all other predictors fixed_.

Example: When it's very hot, ice cream sales go up and car accidents, too.
Does ice cream consumption lead to more accidents?

No. Extremely hot temperatures make people aggressive, leading to more accident-prone behavior.
Probably no big role for ice cream here.


Assessing model performance in multiple linear regression
========================================================

&nbsp;

The $R^2$ above does not work well when we have many predictors: It will only ever increase if we add new variables, however useless these may be.

Other measures like _adjusted R^2_, _AIC_ and other so called _information criteria_ take into account the number of parameters in a model.

Models with more parameters get "penalized".



Predictions from linear regression
========================================================

&nbsp;

Like for any machine learning model, predictions from least squares should always include _prediction intervals_.
If we are predicting averages, not individual examples, _confidence intervals_ will do, too.



Non-linear relationships
========================================================

Often, the relationship between $y$ and $x$ is not linear. 
It is still possible to use linear regression, using non-linear transformations (polynomials, log...) of the predictors - like in this example:

$mpg = \beta_0 + \beta_1 * horsepower + \beta_2 * horsepower^2 + \epsilon$

<figure>
<img src="3.8.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



Lab: Linear regression
========================================================



========================================================


========================================================
