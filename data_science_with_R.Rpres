<style>

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 2em;
}

.reveal h3 {
  font-size: 1.4em;
}

.reveal h4 {
  font-size: 1.2em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.smallcode pre code {
  font-size: 1em;
}

.reveal .smalltext {
  font-size: 0.7em;
}

</style>


Data Science with R
========================================================
author: Sigrid Keydana, Trivadis
date: 
autosize: true
width: 1600


Agenda
========================================================
class:smalltext

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=14, fig.height=6, fig.path='Figs/',
                      warning=FALSE, message=FALSE,
                      cache = TRUE)

```

```{r, results="hide", echo=FALSE}
library(dplyr)
library(readr)
library(Rtsne)
library(ggplot2)
library(GGally)
library(DMwR2)
library(ggthemes)
library(ggExtra)
```


&nbsp;

- Lost in terminology?
- Data science in practice
- Introduction to statistical modeling: supervised vs. unsupervised learning, regression vs. classification, prediction vs. inference 
- R for data science, a short introduction
- Exploratory data analysis 
- Important concepts in statistical modeling: overfitting, bias/variance trade-off, model selection
- Introduction to supervised learning (regression): Linear regression
- Introduction to supervised learning (classification): Logistic regression
- More algorithms: regression and classification trees, support vector machines, k-nearest-neighbors
- Ensemble methods
- Model selection revisited: cross-validation, subset selection, regularization 
- Introduction to unsupervised learning: clustering, principal components analysis (PCA)


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Lost in terminology?
</h1>


The classic: Drew Conway's original Data Science Venn diagram
========================================================

&nbsp;


<figure>
<img src="pics/Data_Science_VD.png">
<figcaption>Source:http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram
</figcaption>
</figure>



One of its many successors...
========================================================

&nbsp;



<figure>
<img src="pics/data-scientist-venn-diagram.png">
<figcaption>Source: http://www.kdnuggets.com/2016/09/new-data-science-venn-diagram.html
</figcaption>
</figure>


... and one more focused on the "doings" 
========================================================

&nbsp;


<figure>
<img src="pics/data-science-puzzle-2017.jpg" width="60%">
<figcaption>Source: http://www.kdnuggets.com/2017/01/data-science-puzzle-revisited.html
</figcaption>
</figure>


Statistics vs. Machine Learning
========================================================

&nbsp;

- different methods?
- or _just_: different goals?
- or _just_: different world views?
- or really: _same old thing_?



Model to black box, and back again? (1)
========================================================

&nbsp;

__Leo Breiman, Statistical Modeling: The Two Cultures (2001)__

&nbsp;


$y = f(x, params, noise)$ &nbsp;&nbsp;&nbsp; vs. &nbsp;&nbsp;&nbsp; $x -> \fbox{black box} -> y$


&nbsp;

- _data modeling_ vs. _algorithmic modeling_

- _goodness of fit_ and _examination of residuals_ vs. _predictive accuracy_ 



Model to black box, and back again? (2)
========================================================

&nbsp;

__Leo Breiman, Statistical Modeling: The Two Cultures (2001)__

&nbsp;

"The goal is not interpretability, but accurate information."



Model to black box, and back again? (3)
========================================================

&nbsp;

__16 years later__

Machine learning / deep learning is taking over decision making in super important areas (or will, soon)

- medical diagnosis
- autonomous driving
- loan applications
- jurisdiction
- ...

We definitely need interpretability / explanations!


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Data Science in practice
</h1>


A more real-world view of data science 
========================================================

&nbsp;

<figure>
<img src="pics/realistic_ds_diagram.png" width="50%">
<figcaption>Source: Jerry Overton, Going Pro in Data Science. O'Reilly 2016.</figcaption>
</figure>


The data science process
========================================================

&nbsp;



<figure>
<img src="pics/data_science_process.png" width="50%">
<figcaption>Source: Jerry Overton, Going Pro in Data Science. O'Reilly 2016.
</figcaption>
</figure>





========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Introduction to statistical modeling
</h1>

<h3>supervised vs unsupervised learning, regression vs. classification, prediction vs. inference 
</h3>


Example: Advertising
========================================================

&nbsp;

Does advertising help sales? How much? 

What about the different channels - should we advertise on TV? Radio? Newspapers?

<figure>
<img src="pics/2.1.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Example: Income
========================================================

&nbsp;

Does income increase with education? How much?

<figure>
<img src="pics/2.2.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>





Example: Clustering gene expression measurements for cancer cells
========================================================

&nbsp;

<figure>
<img src="pics/1.4.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>





Supervised and unsupervised learning
========================================================

&nbsp;

#### Supervised learning

- goal: learn a function from input to output, &nbsp;&nbsp;&nbsp; $y = f(x)$
- workflow: train on _training set_ (and possibly, validation set), test performance on _test set_
- for both training and test set, $x$ and $y$ are given (ground truth)

#### Unsupervised learning

- goal: find patterns (groups, clusters...) in a dataset
- $x$ is given, but $y$ is not


Regression and classification
========================================================

&nbsp;

#### Regression

- output variable is quantitative
    - predict weight from height
    - predict house price from habitable surface and location
    - predict income from parents' income
    
#### Classification

- output variable is qualitative
    - predict credit card default
    - predict disease yes/no
    - predict intervention success


Prediction vs. inference
========================================================

&nbsp;

What's more important: 

- predict a new $\hat y$ as accurately as possible?
- model the underlying relationship between $x$ and $y$ (or even: _explain_ $y$ by $x$?)



Parametric vs. nonparametric methods
========================================================

&nbsp;

#### Parametric

- assume a specific form for $f$ in $y = f(x)$
- e.g., linear regression: $f(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p$
- then, estimate the $\beta_0, \beta_1 ... \beta_p$
- functions with few parameters may be too unflexible to model the true $f$
- functions with many parameters may _overfit_ on the training data (more on overfitting later)

#### Non-parametric

- no explicit assumptions about the underlying $f$
- more flexible than parametric methods, but also more prone to _overfitting_


Prediction accuracy vs. model interpretability
========================================================

&nbsp;

By method

<figure>
<img src="pics/2.7.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
R for data science
</h1>


From S to R
========================================================

&nbsp;

#### Before there was R, there was S

- Statistical computing language developed at Bell Laboratories by John Chambers et al. (1976)
- Major versions S3 and S4
- Mission statement: _turn ideas into software, quickly and faithfully_

#### S-PLUS

- Commercial version of S with eventful history (as of today, integrated into Tibco Spotfire)

#### R

- Developed by Ross Ithaka and Robert Gentleman at University of Auckland (1995-2000)
- Syntax and object system from S, scoping rules and environment model from Scheme 
- Released under GNU Public License (GPL)


The R ecosystem
========================================================

&nbsp;

- CRAN (Comprehensive R Archive Network): repository with more than 10,000 user-contributed packages
- RStudio: most popular R IDE (can do “everything” in RStudio)
- If you can just choose one person to stand for it all: Hadley Wickham (Chief Scientist at RStudio), creator of the _tidyverse_
- Books: many. Recommended e.g.: <a href="http://r4ds.had.co.nz/">_R for Data Science_</a> by Hadley Wickham,
<a href="http://adv-r.hadley.nz/">Advanced R</a> also by Hadley Wickham, <a href="https://bookdown.org/rdpeng/RProgDA/">Mastering software development in R</a> by Roger Peng


Why R (for data science)
========================================================

&nbsp;

- basically every statistical or machine learning algorithm is implemented in R
- even if you have to implement a new algorithm yourself, you can build upon a strong fundament (optimization, matrix computations ...)
- elegant, concise syntax, esp. when using packages from the _tidyverse_
- quickly try out things and experiment on the (RStudio) Console
- beautiful graphics
- (personal opinion) programming in R is fun!


Getting started with R (1)
========================================================
class:smallcode

&nbsp;

#### Basic data types and assignment

&nbsp;

```{r, results="hide" }
2      # a number
class(2)
2.0    # the same number
class(2.0)

"two"  # a string (character vector)
'two'  # can also use single quotes

TRUE == FALSE # boolean datatype, called logical in R

x <- 1 # assignment
x
y = 2 # can also use =

```



Getting started with R (2)
========================================================
class:smallcode

&nbsp;

#### 1-dimensional data structures: vectors and lists

&nbsp;


```{r, results="hide" }
# Vectors: all elements of a vector must be of the same type
c(1,2,3) # a vector of doubles
c("a", "b", "c") # a vector of character strings
1:10 # a shortcut 
seq(1, 10, by=0.5)

# actually, all objects are non-scalars, there is no real difference
length(c(1,2,3))
length(1)

# a factor (for categorical data)
f <- factor(c("a", "b", "b")) 
f
levels(f)

# Lists may have elements of different types
list(1, 2, 3, FALSE, 'x', 'y','z')
# list elements may be , and often are, named:
list(numbers = 1:3, logical = FALSE, letters = letters[24:26])

```


Getting started with R (3)
========================================================
class:smallcode

&nbsp;

#### 2-dimensional data structures: matrices and data frames

&nbsp;


```{r, results="hide" }
# Matrices are like vectors in that the elements have to be of the same type
matrix(1:20, nrow = 2, ncol =10)
matrix(1:20, nrow = 2, ncol =10, byrow = TRUE)

# data frames are like lists in their columns may be of different types
data.frame(firstcol = 1:3, secondcol = rep(777,3), thirdcol = c('desc', 'anotherdesc', 'yetanotherdesc'))

# R comes with many datasets included
data("mtcars")
mtcars
dim(mtcars)

# if we need matrices of more than 2 dimensions, we use arrays:
array(1:64, dim = c(2,8,4))
```


Getting started with R (4)
========================================================
class:smallcode

&nbsp;

#### Subsetting: vectors and lists

&nbsp;

```{r, results="hide" }
# subsetting a vector
v <- 1:10
v[2]
v[3:4]

# subsetting a list
mylist <- list(LETTERS[1:4], rep(1,10), 7777) 
mylist
mylist[3]; class(mylist[3]) # this is a sublist
mylist[[3]]; class(mylist[[3]]) # this is the element by itself

# a named list
mylist <- list(chars = LETTERS[1:4], numbers = rep(1,10), one_number = 7777)
mylist
mylist$chars # way 1 
mylist[["chars"]] # way 2
```


Getting started with R (5)
========================================================
class:smallcode

&nbsp;

#### Subsetting: matrices and data frames

&nbsp;

```{r, results="hide" }
# subsetting a matrix
m <- matrix(1:12, 2, 6)
m[1,2]
m[1, ]
m[ ,1]
m[1, 3:5]

# subsetting a data frame
mtcars
mtcars$mpg
mtcars[["mpg"]]
mtcars$cyl[2]
```


Getting started with R (6)
========================================================
class:smallcode

&nbsp;

#### The tidyverse: data manipulation with dplyr

&nbsp;

```{r, results="hide" }
library(dplyr)
library(ggplot2)
data(diamonds)

# SQL WHERE
filter(diamonds, cut == "Premium")

# using the pipe from now on
diamonds %>% filter(cut == "Premium")
diamonds %>% filter(cut == "Premium" & depth > 62.4)

# SQL SELECT
diamonds %>% filter(cut == "Premium" & depth > 62.4) %>% select(price)

# SQL group by
diamonds %>% filter(cut == "Premium" & depth > 62.4) %>% group_by(color) %>% summarise(avg_price = mean(price))

```


Getting started with R (7)
========================================================
class:smallcode

&nbsp;

#### The tidyverse: plotting with ggplot2

&nbsp;

```{r, results="hide", fig.keep="none"}
library(ggplot2)
data(diamonds)

# a basic scatterplot
ggplot(data = diamonds, mapping = aes(x = carat, y = price)) + geom_point()

# adding smoothing
ggplot(data = diamonds, mapping = aes(x = carat, y = price)) + geom_point() + geom_smooth()
 
# boxplots
ggplot(diamonds, aes(x = 1, y = carat)) + geom_boxplot()
ggplot(diamonds, aes(x = cut, y = carat)) + geom_boxplot()

ggplot(diamonds, aes(x = cut, y = carat)) + geom_violin()
```


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Exploratory data analysis
</h1>


The most important thing about exploratory data analysis
========================================================

&nbsp;

&nbsp;

&nbsp;

__Do it.__


66% of variance in income can be explained by tenure!
========================================================
class:smallcode

&nbsp;

... as shown by $R^2$ in 4 datasets:

```{r}
# spoiler... this is not about income, this is the famous Anscombe dataset
data("anscombe")
summary(lm(y1 ~ x1, data = anscombe))$r.squared
summary(lm(y2 ~ x2, data = anscombe))$r.squared
summary(lm(y3 ~ x3, data = anscombe))$r.squared
summary(lm(y4 ~ x4, data = anscombe))$r.squared
```



How does the data really look?
========================================================
incremental:true

&nbsp;

```{r, echo=FALSE}
data("anscombe")
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
}

op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13))
  abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)
```



Getting to know the dataset (1)
========================================================
class:smallcode

&nbsp;

What kind of data do we have?

```{r}
data("algae")
nrow(algae)
str(algae)
```



Getting to know the dataset (2)
========================================================
class:smallcode

&nbsp;

Summary statistics:

```{r}
summary(algae)
```


Getting to know the dataset (3)
========================================================
class:smallcode

&nbsp;

Find missing values:

```{r}
algae %>% summarise_all(funs(sum(is.na(.))))
```



Getting to know the dataset (4)
========================================================
class:smallcode

&nbsp;

Correlation matrix:

```{r}
algae %>% select_if(is.numeric) %>% filter_all(all_vars(!is.na(.))) %>% cor()
```


Getting to know the dataset (5)
========================================================
class:smallcode

&nbsp;

Pairs plot:

```{r}
algae %>% select_if(is.numeric) %>% select(-(num_range("a", 2:7))) %>% ggpairs()
```



Goals of exploration
========================================================

&nbsp; 

Further exploration will depend on your questions / hypothesis.

In any case, you will probably want to summarize the variables you're interested in.


What is a good summary?
========================================================

&nbsp;

Mean vs. median (1)

```{r, echo=FALSE}
bold_text_20 <- element_text(face = "bold", size = 20)
ggplot(algae, aes(x = mxPH)) + geom_density(na.rm = TRUE) +
  geom_vline(aes(
  xintercept = mean(algae$mxPH, na.rm = TRUE),
  linetype = "mean"
  ), color = 'red') +
  geom_vline(aes(
  xintercept = median(algae$mxPH, na.rm = TRUE),
  linetype = "median"
  ), color = 'green') +
  scale_linetype_manual(
  name = "statistic",
  values = c(2, 2),
  guide = guide_legend(override.aes = list(color = c("red", "green")))
  ) +
  xlab("") + ylab("density") +
ggtitle("algae$mxPH") + theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)
```


What is a good summary?
========================================================

&nbsp;

Mean vs. median (2)


```{r, echo=FALSE}
bold_text_20 <- element_text(face = "bold", size = 20)
ggplot(algae, aes(x = a1)) + geom_density(na.rm = TRUE) +
  geom_vline(aes(
  xintercept = mean(algae$a1, na.rm = TRUE),
  linetype = "mean"
  ), color = "red") +
  geom_vline(aes(
  xintercept = median(algae$a1, na.rm = TRUE),
  linetype = "median"
  ), color = "green") +
  scale_linetype_manual(
  name = "statistic",
  values = c(2, 2),
  guide = guide_legend(override.aes = list(color = c("red", "green")))
  ) +
  xlab("") + ylab("density") +
ggtitle("algae$a1") + theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)
```


So should we always go with the median?
========================================================

&nbsp;

```{r, echo=FALSE}
betas <- rbeta(100,.5,.5)
ggplot(data.frame(x=betas), aes(x = x)) + geom_histogram(bins = 20) +
  geom_vline(aes(xintercept = mean(x), linetype = "mean"), color = 'red') + 
  geom_vline(aes(xintercept = median(x), linetype = "median"), color = 'green') + 
  scale_linetype_manual(name = "statistic", values = c(2, 2), 
                      guide = guide_legend(override.aes = list(color = c("red", "green")))) +
  ylab("count")  + ggtitle("A bimodal distribution") + theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)
```


Boxplots are a way to summarize a complete distribution
========================================================

&nbsp;

The median is the fat black line. The green and blue lines have been added to indicate the mean and the mode.
Do you know which is which?

```{r, echo=FALSE}
mod <-algae %>% group_by(a1) %>% summarize(n = n()) %>% arrange(desc(n)) %>% select(a1) %>% head(1)
ggplot(algae, aes(x = 1, y = a1))  + geom_boxplot() + 
  geom_hline(aes(yintercept = mean(algae$a1)), color = "green", linetype=2) + 
  geom_hline(aes(yintercept = mod), color = "blue", linetype=4)  + 
  ylab("")  +
ggtitle("algae$a1") + theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)
```


Does a boxplot always help?
========================================================

&nbsp;

This is a boxplot for the bimodal distribution a few slides ago.

We've added a rug on the left to show where the data points lie.
This would be hard to guess from the boxplot alone.

```{r, echo=FALSE, fig.width=12, fig.height=4}
 ggplot(data_frame(y = betas), aes(x = 1, y = betas)) + geom_boxplot() + geom_rug() +
  ylab("") + ggtitle("A bimodal distribution") + theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)

```


Bringing out the music in data - violin plots
========================================================

&nbsp;

```{r, echo=FALSE, fig.height=8, fig.width = 8}
 ggplot(data_frame(y = betas), aes(x = 1, y = betas)) + geom_violin() + geom_rug() + 
  ylab("") + ggtitle("A bimodal distribution") + theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)

```


Visualizing relations with scatterplots
========================================================

&nbsp;

A scatterplot shows how two variables relate to each other.<br />
Density curves, plotted parallel to the x- and y axes, can help to keep track of the univariate densities.


```{r, echo=FALSE, fig.width=6, fig.height=6}
g1 <- ggplot(algae, aes(x = oPO4, y = a1)) + geom_point() +
  theme_tufte(ticks=F) + theme(axis.title=element_blank(), axis.text=element_blank())
g1 <- g1 %>% ggMarginal(type = "density") 
g1
```


How can I visualize multidimensional data?
========================================================

&nbsp;

t-SNE (_t-Distributed Stochastic Neighbor Embedding_) is a relatively new, effectful visualization algorithm especially suited for non-linear data.

The example data shown below are satellite data, with 36 predictors available to predict the nature of the ground.

```{r, echo=FALSE, fig.width=10, fig.height=4}
# https://archive.ics.uci.edu/ml/datasets/Statlog+%28Landsat+Satellite%29
sat <- read_delim("data/sat.trn", delim = " ", col_names = FALSE)
sat_data <- sat %>% select(-X37)
tsne_out <- Rtsne(sat_data) 
plot(tsne_out$Y,col=sat$X37, xlab = "", ylab="", xaxt='n', yaxt='n')
```


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Important concepts in statistical modeling
</h1>
<h3>Overfitting, bias/variance trade-off, and model selection
</h3>


Training error, test error, and overfitting
========================================================
class:smalltext

&nbsp;

A model that minimizes training error but incurs high test error is said to _overfit_ the training data.

While good performance on the training set is nice, the thing we really care about is _low error on the test set_ (low _generalization error_).

<figure>
<img src="pics/2.9.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

There is no such thing as an overall optimal complexity (1)
========================================================
class:smalltext

&nbsp;

Three unequal-complexity models with training and test errors

_moderately nonlinear data_

<figure>
<img src="pics/2.9.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

There is no such thing as an overall optimal complexity (2)
========================================================
class:smalltext

&nbsp;

Three unequal-complexity models with training and test errors

_highly linear data_

<figure>
<img src="pics/2.10.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

There is no such thing as an overall optimal complexity (3)
========================================================
class:smalltext

&nbsp;

Three unequal-complexity models with training and test errors

_very nonlinear data_

<figure>
<img src="pics/2.11.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

How to prevent overfitting
========================================================

&nbsp;

- use cross-validation to determine optimal model complexity 
- penalize more complex models (regularization)

We'll cover cross validation below and see regularization later.


Bias-variance tradeoff (1)
========================================================

&nbsp;

The expected test error decomposes into 3 separate components:

- the _variance_ of the estimator due to sample variation
- the _bias_ due to approximating the true relationship with a simplification
- the noise variance (irreducible error)

&nbsp;

$E(y_0 - \hat f(x_0))^2 = var(\hat f(x_0)) + (bias(\hat f(x_0)))^2 + var(\epsilon)$


Bias-variance tradeoff (2)
========================================================
class:smalltext

&nbsp;

<figure>
<img src="pics/2.12.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Test MSE, with its three components variance, bias and irreducible error, vs. _flexibility_, for the three scenarios above. 
Dashed line is irreducible error.


Model selection
========================================================

&nbsp;

How do we select the best model?

The most common approach is _cross-validation_.

(Another is imposing priors / using regularization).


How cross-validation works
========================================================

&nbsp;

Example: 5-fold cross-validation.

<figure>
<img src="pics/5.5.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Supervised learning (regression): linear regression
</h1>


What is the effect of advertising on sales?
========================================================

&nbsp;

In simple (= single-predictor) linear regression we fit a line to the data:

$\hat y = \beta_0 + \beta_1 x$

<figure>
<img src="pics/2.1.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


How do we estimate the coefficients?
========================================================

&nbsp;

The most common approach by far is _least squares_ where we minimize the _residual sum of squares_

$(y_1 - \hat \beta_0 - \hat \beta_1 x_1)^2 + (y_2 - \hat \beta_0 - \hat \beta_1 x_2)^2 + ... + (y_n - \hat \beta_0 - \hat \beta_1 x_n)^2$

<figure>
<img src="pics/3.1.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Which gives the estimates
========================================================

&nbsp;


$\hat \beta_0 = \bar y - \hat \beta_1 \bar x$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

$\hat \beta_1 = cor(y, x) \frac{sd(y)}{sd(x)}$

&nbsp;

That means, the slope of the line

- gets steeper when $x$ and $y$ are more correlated
- gets steeper when there's more variation in the $y$'s
- gets shallower when there's more variation in the $x$'s

How good is the model fit?
========================================================

&nbsp;

Model performance in the one-predictor case is usually assessed by calculating the proportion of variance explained:

$R^2 = \frac{total\: sum\: of\: squares\: -\: residual\: sum\: of\: squares\:}{total\: sum\: of\: squares\:} = 1 - \frac{residual\: sum\: of\: squares}{total\: sum\: of\: squares\:}$

This $R^2$ is the square of the correlation coefficient $R$.


Multiple predictors
========================================================

&nbsp;

With n predictors, we don't fit a line any more, but an n-dimensional hyperplane.

This is an example of the regression of a hypothetical $y$ on two hypothetical predictors $x_1$ and $x_2$.

<figure>
<img src="pics/3.4.jpg" width="40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


What it means
========================================================
incremental:true

&nbsp;

In multiple regression, a predictor is tested for its impact _holding all other predictors fixed_.

Example: When it's very hot, ice cream sales go up and car accidents, too.
Does ice cream consumption lead to more accidents?

No. Extremely hot temperatures make people aggressive, leading to more accident-prone behavior.
Probably no big role for ice cream here.


Assessing model performance in multiple linear regression
========================================================

&nbsp;

The $R^2$ above does not work well when we have many predictors: It will only ever increase if we add new variables, however useless these may be.

Other measures like _adjusted R^2_, _AIC_ and other so called _information criteria_ take into account the number of parameters in a model.

Models with more parameters get "penalized".



Predictions from linear regression
========================================================

&nbsp;

Like for any machine learning model, predictions from least squares should always include _prediction intervals_.

If we are predicting averages, not individual examples, _confidence intervals_ will do, too.



Non-linear relationships
========================================================

Often, the relationship between $y$ and $x$ is not linear. 
It is still possible to use linear regression, using non-linear transformations (polynomials, log...) of the predictors - like in this example:

$mpg = \beta_0 + \beta_1 * horsepower + \beta_2 * horsepower^2 + \epsilon$

<figure>
<img src="pics/3.8.jpg" width=40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



Linear regression
========================================================

&nbsp;

- lab_linear_regression.R
- exercises_linear_regression.R


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Supervised learning (classification): logistic regression
</h1>


Who will default on their credit card?
========================================================

&nbsp;

Why we cannot just use linear regression

- on binary data, in principle we could, coding the response as 0/1
- but we would obtain some strange predictions...

<figure>
<img src="pics/4.2.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Logistic regression - rationale
========================================================

&nbsp;

- Instead of the response itself (that is directly modeled by linear regression), logistic regression models the probability of the outcome

$P(default = Yes |  balance)$

- probabilities are between 0 and 1 - one function that does this is the _logistic_:

$p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}$

- in logistic regression, what corresponds to the regression coefficient(s) is the _log-odds_ or _logit_:

$log(\frac{p(X)}{1-p(X)}) = \beta_0 + \beta_1 X$

- this means that with a one-unit increase in X is associated a 1-unit increase in the _log-odds_ of value $\beta_1$


In classification, not all errors are born equal 
========================================================

&nbsp;

- the simplest measure of success in classification is _accuracy_, the proportion of correct classifications 

$accuracy = \frac{number\>correctly\>classified}{classifications\>overall}$

- however, errors are either false positives or false negatives, and often it may make a big difference which it is!
- therefore, we want to look at the _confusion matrix_ that shows all 4 possibilities
- what would you think of the following confusion matrix for credit card default data?

<figure>
<img src="pics/table_4_4.png" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Terminology
========================================================
class:smalltext

&nbsp;

action/truth                | $\mathbf{H_0}$ is true   | $\mathbf{H_1}$ is true   | total
-------------               | -------------            | -------------            | ---
__$\mathbf{H_1}$ accepted__ | $V$                      | $S$                      | $R$
__$\mathbf{H_1}$ rejected__ | $U$                      | $T$                      | $n - R$
__total__                   | $n_0$                    | $n - n_0$                | $n$


* False Positive / __Type 1 error__: $V$
* False Negative / __Type 2 error__: $T$
* __Sensitivity__ / __Recall__ / True Positive Rate: proportion of correctly detected positives, $\frac{S}{n - n_0}$
* __Specificity__ / True Negative Rate: proportion of negatives correctly identified as such, $\frac{U}{n_0}$
* __Precision__: proportion of accepted instances that are correct, $\frac{S}{R}$ (= Positive Predictive Value)
* __Accuracy__: proportion classified correctly, $\frac{S + U}{n}$

(after: http://en.wikipedia.org/wiki/Familywise_error_rate)


ROC Curves
========================================================

&nbsp;

The _area under the ROC curve_ (_AUC_) measures the overall performance of a classifier.

<figure>
<img src="pics/4.8.jpg" width="40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Decision boundaries
========================================================

&nbsp;

In two dimensions, it is possible to display a classifier's decision boundary.

Here are decision boundaries for two classifiers, a linear and a non-linear one (the dashed line representing the theoretical optimum):

<figure>
<img src="pics/4.9.jpg" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Logistic regression
========================================================

&nbsp;

- lab_logistic_regression.R
- exercises_logistic_regression.R


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Algorithms
</h1>
<h3>
Regression and classification trees, support vector machines, k-nearest-neighbors
</h3>

Decision trees
========================================================

&nbsp;

- Decision trees partition the dataset into regions based on the predictors.
- They can be used for classification and regression.

A simple regression tree for prediction of (log) salary of baseball players:

<figure>
<img src="pics/8.1.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Regression trees - rationale
========================================================

&nbsp;

- Regression trees divide the predictor space into regions/boxes and then predict the same value for every region/box.
- The goal is to find boxes that minimize the RSS, that is, the sum of the squared deviations of every single $y$ in a box from its "box mean" (summed over all box members and all boxes)

Here is the partitioning into regions for the baseball players example:

<figure>
<img src="pics/8.2.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



Danger zone!
========================================================

&nbsp;

If we follow that strategy blindly, we end up _overfitting_ the dataset, as with this tree:

<figure>
<img src="pics/8.4.jpg" width="40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Cost complexity pruning
========================================================
class:smalltext

&nbsp;

With cost complexity pruning, we obtain the optimal tree size trading off complexity (defined as number of terminal nodes) against the RSS.

$$\sum_{m=1}^{|T|} \sum_{i: x_i \epsilon R_m} (y_i - \hat y_{R_m})^2 + \alpha|T|$$

<figure>
<img src="pics/tree_algo.png" width="50%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Classification trees
========================================================

&nbsp;

- Classification trees are constructed analogously to regression trees
- apart from their using a different error measure: mostly one of
    - Gini index (= total variance across classes, inhomogeneity/_impurity_ of nodes)
    - cross entropy (practically equivalent in this context)



Classification tree example: Detecting heart disease
========================================================

&nbsp;

<figure>
<img src="pics/8.6.jpg" width=45%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Trees versus linear models
========================================================

&nbsp;

- Trees are easily interpretable and optimally suited for communication
- linear models are often more robust to changes in the sample dataset, and more accurate in their predictions

In a concrete case, the shape of the true decision boundary plays an important role:

<figure>
<img src="pics/8.7.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



Decision trees
========================================================

&nbsp;

- lab_decision_trees.R


Support vector machines
========================================================

&nbsp;

- Support vector machines have been _the hype_ out there until the event of deep learning.
- They combine the _maximal margin_ principle with _kernels_ that allow for handling all kinds of non-linear data.


Separating hyperplanes
========================================================

&nbsp;

In this example, the hyperplane $1 + 2X_1 + 3X_2 = 0$ separates the blue and the red regions.

For all red points, $1 + 2X_1 + 3X_2 < 0$, while for all blue points, $1 + 2X_1 + 3X_2 > 0$.


<figure>
<img src="pics/9.1.jpg" width="20%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

But there are many potential separating hyperplanes...
========================================================

&nbsp;

<figure>
<img src="pics/9.2.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Linearly separable datasets: the maximal margin classifier
========================================================

&nbsp;

The maximal margin hyperplane depends only on the _support vectors_ (shown with arrows), not on any other observations from the dataset!

<figure>
<img src="pics/9.3.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Moving away from the linear separability condition
========================================================

&nbsp;

Most datasets are not linearly separable.

In fact, even if they are, a separating hyperplane may not always be the right choice!

<figure>
<img src="pics/9.5.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



The support vector classifier
========================================================

&nbsp;

The support vector classifier allows for some points to be misclassified, subject to a "misclassification budget" set by a tuning parameter.

Observations may be on the wrong side of the margin, and even on the wrong side of the hyperplane:

<figure>
<img src="pics/9.6.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Support vectors and the bias-variance trade-off
========================================================

&nbsp;

Depending on the tuning parameter for the "misclassification budget", the resulting classifier takes a different position in the bias-variance trade-off:

- big "misclassification budget": many support vectors, low variance, potentially high bias
- small "misclassification budget": few support vectors, high variance, low bias

<figure>
<img src="pics/9.7.jpg" width="20%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


From support vector classifier to support vector machine
========================================================

&nbsp;

No linear classifier can possibly account for this scenario...

<figure>
<img src="pics/9.8.jpg" width="20%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


The support vector machine
========================================================

&nbsp;

Support vector machines model non-linear decision boundaries using _kernels_ (such as polynomial, radial...)

This is how SVMs with polynomial resp. radial kernel handle the above problem:

<figure>
<img src="pics/9.9.jpg" width="20%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Support vector machine
========================================================

&nbsp;

- lab_svm.R


K-nearest neighbors (KNN)
========================================================

&nbsp;

K-nearest neighbors is a non-parametric algorithm that can be used for classification as well as regression.

A test point is classified (or assigned a prediction value) according to the k _nearest_ (by Euclidean distance, mostly) exemplars in the training set.

Note: there are no separate training and test phases!

<figure>
<img src="pics/knn.png" width="10%">
<figcaption>Source: Wikipedia.
</figcaption>
</figure>


KNN: decision boundaries
========================================================

&nbsp;

The KNN decision boundary (and thus, the position taken in the bias-variance trade-off) depends on the parameter k.

Left: k=1, right: k=100

<figure>
<img src="pics/2.16.jpg" width="40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

K-nearest neighbors (KNN)
========================================================

&nbsp;

- lab_knn_classification.R
- exercises_algorithms.R


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Ensemble methods
</h1>


Many trees make up a forest
========================================================

&nbsp;

Among the most successful methods in practice are ensemble methods that _average over many runs of an algorithm_.

Where a single decision tree lacks robustness (has high variance), many trees - as in _bagging_, _boosting_ and _random forests_ - are both stable and accurate.

Bagging
========================================================

&nbsp;

- generate B bootstrapped training sets
- train a tree on every one of them and collect the predictions
- average the predictions


Random forests
========================================================

&nbsp;

- generate B bootstrapped training sets
- train a tree on every one of them
- at each step, consider just a _random subset_ of predictors for the split
- collect and average the predictions, as with bagging

As opposed to bagging, random forests build averages over more dissimilar trees, leading to a higher reduction in variance.


Boosting
========================================================

&nbsp;

- does not involve bootstrap sampling
- instead, builds a sequence of decision trees, where each new tree improves over the weaknesses of the previous ones
- i.e., builds a succession of "slow learners"
- each single tree often has very small depth (up to "decision stumps")

How about interpretability?
========================================================

&nbsp;


Even though with ensemble methods like random forests we lose the ease of visual inspection, we can still obtain a graph of _variable importance_, as in this example:

<figure>
<img src="pics/8.9.jpg" width="30%">
<figcaption>Source: G. James, D. Witten, T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Ensemble methods
========================================================

&nbsp;

- lab_ensemble_methods.R


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Model selection revisited
</h1>

<h3>Cross-validation, subset selection and regularization</h3>

Cross-validation
========================================================

&nbsp;

- lab_cross_validation.R


Subset selection
========================================================

&nbsp;

- lab_subset_selection.R


Regularization
========================================================

&nbsp;

- lab_ridge_lasso.R

========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Introduction to unsupervised learning
</h1>
<h3>Clustering and Principal Components Analysis
</h3>

K-means clustering
========================================================

&nbsp;

Idea: Within-cluster variation should be small.

However, as it's unsupervised learning, the number of clusters is not a given!

In this example, what should k be?

<figure>
<img src="pics/10.5.jpg" width="40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

K-means algorithm
========================================================

&nbsp;

Alternating steps:
- compute cluster centroid
- assign data points to nearest cluster (by Euclidean distance)

<figure>
<img src="pics/10.6.jpg" width="25%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Local optima
========================================================

&nbsp;

Random initialization of cluster assignments affects the outcome.

Here are the results from six runs of K-means. In four cases, a local optimum was reached.


<figure>
<img src="pics/10.7.jpg" width="25%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Hierarchical Clustering
========================================================

&nbsp;

With hierarchical clustering, we don't have to decide on the number of clusters in advance.

We can cut the dendrogram (that results from the clustering algorithm) at different places:

<figure>
<img src="pics/10.9.jpg" width="40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


How to read a dendrogram
========================================================

&nbsp;

Items that are near each other _vertically_ are similar.

_Horizontal proximity_ on the other hand says nothing about similarity!

<figure>
<img src="pics/10.10.jpg" width="40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Hierarchical Clustering - algorithm
========================================================

&nbsp;

Illustration of the algorithm (complete linkage, Euclidean distance):

<figure>
<img src="pics/10.11.jpg" width="25%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Hierarchical Clustering - linkage
========================================================

&nbsp;

How distance is measured makes a difference.

Clusters resulting from average, complete, and single linkage, respectively:

<figure>
<img src="pics/10.12.jpg" width="40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Clustering
========================================================

&nbsp;

- lab_clustering.R


Principal components analysis
========================================================

&nbsp;

PCA finds an equivalent representation of the data where each _principal component_ explains a maximal amount of variance and is orthogonal to all other principal components.

For explanation and display, we often focus on just the first few principal components:

<figure>
<img src="pics/10.1.jpg" width="35%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


How many principal components to discuss?
========================================================

&nbsp;

Scree plots show the proportion of variance explained by successive principal components.

<figure>
<img src="pics/10.4.jpg" width="40%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Principal Components Analysis
========================================================

&nbsp;

- lab_pca.R

