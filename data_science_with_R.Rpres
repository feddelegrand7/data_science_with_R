<style>

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 2em;
}

.reveal h3 {
  font-size: 1.4em;
}

.reveal h4 {
  font-size: 1.2em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.smallcode pre code {
  font-size: 1em;
}

.reveal .smalltext {
  font-size: 0.8em;
}

</style>


Data Science with R
========================================================
author: Sigrid Keydana, Trivadis
date: 
autosize: true
width: 1600


Agenda
========================================================
class:smalltext

&nbsp;

### Part 1: Concepts, basics, methods

1. Statistics, data mining, predictive analytics, data science, machine learning, deep learning... what???
2. Introduction to statistical modeling: supervised vs. unsupervised, regression vs. classification, prediction vs. inference 
3. R for data science
4. Exploratory data analysis and visualization (modelr & hadley case studies???)
5. Important concepts in statistical modeling: overfitting, bias/variance and model selection
6. Some statistical background
7. Introduction to supervised learning (1): Linear regression
8. Introduction to supervised learning (2): Logistic regression
9. Algorithms: regression and classification trees, support vector machines, k-nearest-neighbors
10. Ensemble methods
11. Model selection revisited: Cross validation, subset selection and regularization (labs!!!)
11. Introduction to unsupervised learning: clustering, principal components analysis

### Part 2: Case study: from data to models

fraud  detection


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Introduction to statistical modeling
</h1>


Example: Advertising
========================================================

&nbsp;

Does advertising help sales? How much? What about the different channels - should we advertise on TV? Radio? Newspapers?

<figure>
<img src="2.1.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Example: Income
========================================================

&nbsp;

Does income increase with education? How much?

<figure>
<img src="2.2.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>





Example: Clustering gene expression measurements for cancer cells
========================================================

&nbsp;

<figure>
<img src="1.4.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>





Supervised and unsupervised learning
========================================================

&nbsp;

#### Supervised learning

- goal: learn a function from input to output, &nbsp;&nbsp;&nbsp; $y = f(x)$
- workflow: train on _training set_ (and possibly, validation set), test performance on _test set_
- for both training and test set, $x$ and $y$ are given (ground truth)

#### Unsupervised learning

- goal: find patterns (groups, clusters...) in a dataset
- $x$ is given, but $y$ is not


Regression and classification
========================================================

&nbsp;

#### Regression

- output variable is quantitative
    - predict weight from height
    - predict house price from habitable surface and location
    - predict income from parents' income
    
#### Classification

- output variable is qualitative
    - predict credit card default
    - predict disease yes/no
    - predict intervention success


Prediction vs. inference
========================================================

&nbsp;

What's more important: 

- predict a new $\hat y$ as accurately as possible?
- model the underlying relationship between $x$ and $y$ (or even: _explain_ $y$ by $x$?)



Parametric vs. nonparametric methods
========================================================

&nbsp;

#### Parametric

- assume a specific form for $f$ in $y = f(x)$
- e.g., linear regression: $f(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p$
- then, estimate the $\beta_0, \beta_1 ... \beta_p$
- functions with few parameters may be too unflexible to model the true $f$
- functions with many parameters may _overfit_ on the training data (more on overfitting later)

#### Non-parametric

- no explicit assumptions about the underlying $f$
- more flexible than parametric methods, but also more prone to _overfitting_


Prediction accuracy vs. model interpretability
========================================================

&nbsp;

By method

<figure>
<img src="2.7.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



========================================================

========================================================

========================================================

========================================================

========================================================




========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
R for data science
</h1>


From S to R
========================================================
class:smalltext

&nbsp;

#### Before there was R, there was S

- Statistical computing language developed at Bell Laboratories by John Chambers et al. (1976)
- Major versions S3 and S4
- Mission statement: _turn ideas into software, quickly and faithfully_

#### S-PLUS

- Commercial version of S with eventful history (as of today, integrated into Tibco Spotfire)

#### R

- Developed by Ross Ithaka and Robert Gentleman at University of Auckland (1995-2000)
- Syntax and object system from S, scoping rules and environment model from Scheme 
- Released under GNU Public License (GPL)


The R ecosystem
========================================================

&nbsp;

- CRAN (Comprehensive R Archive Network): repository with more than 10,000 user-contributed packages
- RStudio: most popular R IDE (can do “everything” in RStudio)
- If you can just choose one person to stand for it all: Hadley Wickham (Chief Scientist at RStudio), creator of the _tidyverse_
- Books: many. We will partly use examples from _R for Data Science_ by the above Hadley Wickham 


Why R (for data science)
========================================================

&nbsp;

- basically every statistical or machine learning algorithm is implemented in R
- even if you have to implement a new algorithm yourself, you can build upon a strong fundament (optimization, matrix computations ...)
- elegant, concise syntax, esp. when using packages from the _tidyverse_
- quickly try out things and experiment on the (RStudio) Console
- beautiful graphics
- (personal opinion) programming R is fun!


Getting started with R (1)
========================================================
class:smallcode

&nbsp;

#### Basic data types and assignment

```{r, results="hide" }
2      # a number
class(2)
2.0    # the same number
class(2.0)

"two"  # a string (character vector)
'two'  # can also use single quotes

TRUE == FALSE # boolean datatype, called logical in R

x <- 1 # assignment
x
y = 2 # can also use =

```


Getting started with R (2)
========================================================
class:smallcode

&nbsp;

#### Data structure

```{r, results="hide" }
z <- c(1,2,3) # a vector of doubles
zz <- c("a", "b", "c") # a vector of character strings

# actually, all objects are non-scalars, there is no real difference
length(c(1,2,3))


zzz <- factor(zz) # a factor (for categorical data)

```




========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Important concepts in statistical modeling
</h1>


Training error, test error, and overfitting
========================================================

&nbsp;

A model that minimizes training error but incurs high test error is said to _overfit_ the training data.

While good performance on the training set is nice, the thing we really care about is _low error on the test set_ (low _generalization error_).

<figure>
<img src="2.9.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

There is no such thing as overall optimal complexity (1)
========================================================

&nbsp;

Three unequal-complexity models with training and test errors

_moderately nonlinear data_

<figure>
<img src="2.9.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

There is no such thing as overall optimal complexity (2)
========================================================

&nbsp;

Three unequal-complexity models with training and test errors

_highly linear data_

<figure>
<img src="2.10.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

There is no such thing as overall optimal complexity (3)
========================================================

&nbsp;

Three unequal-complexity models with training and test errors

_very nonlinear data_

<figure>
<img src="2.11.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Example: three fits (true data in black) with corresponding training and test errors

How to prevent overfitting
========================================================

&nbsp;

- use cross-validation to determine optimal model complexity 
- penalize more complex models (regularization)

We'll cover cross validation below and see regularization later.


Bias-variance tradeoff (1)
========================================================

&nbsp;

The expected test error decomposes into 3 separate components:

- the _variance_ of the estimator due to sample variation
- the _bias_ due to approximating the true relationship with a simplification
- the noise variance (irreducible error)

&nbsp;

$E(y_0 - \hat f(x_0))^2 = var(\hat f(x_0)) + (bias(\hat f(x_0)))^2 + var(\epsilon)$


Bias-variance tradeoff (2)
========================================================

&nbsp;

<figure>
<img src="2.12.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Test MSE, with its three components variance, bias and irreducible error, vs. _flexibility_, for the three scenarios above. 
Dashed line is irreducible error.


Model selection
========================================================

&nbsp;

How do we select the best model?

The most common approach is _cross-validation_.

(Another is imposing priors / using regularization).


How cross-validation works
========================================================

&nbsp;

Example: 5-fold cross-validation.

<figure>
<img src="5.5.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Supervised learning (regression): linear regression
</h1>


What is the effect of advertising on sales?
========================================================

&nbsp;

In simple (= single-predictor) linear regression we fit a line to the data:

$\hat y = \beta_0 + \beta_1 x$

<figure>
<img src="2.1.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


How do we estimate the coefficients?
========================================================

&nbsp;

The most common approach by far is _least squares_ where we minimize the _residual sum of squares_

$(y_1 - \hat \beta_0 - \hat \beta_1 x_1)^2 + (y_2 - \hat \beta_0 - \hat \beta_1 x_2)^2 + ... + (y_n - \hat \beta_0 - \hat \beta_1 x_n)^2$

<figure>
<img src="3.1.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Which gives the estimates
========================================================

&nbsp;


$\hat \beta_0 = \bar y - \hat \beta_1 \bar x$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

$\hat \beta_1 = cor(y, x) \frac{sd(y)}{sd(x)}$

&nbsp;

That means, the slope of the line

- gets steeper when $x$ and $y$ are more correlated
- gets steeper when there's more variation in the $y$'s
- gets shallower when there's more variation in the $x$'s

How good is the model fit?
========================================================

&nbsp;

Model performance in the one-predictor case is usually assessed by calculating the proportion of variance explained:

$R^2 = \frac{total sum of squares - residual sum of squares}{total sum of squares} = 1 - \frac{residual sum of squares}{total sum of squares}$

This $R^2$ is the square of the correlation coefficient $R$.


Multiple predictors
========================================================

&nbsp;

With n predictors, we don't fit a line any more, but an n-dimensional hyperplane.

This is an example of the regression of a hypothetical $y$ on two hypothetical predictors $x_1$ and $x_2$.

<figure>
<img src="3.4.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


What it means
========================================================

&nbsp;

In multiple regression, a predictor is tested for its impact _holding all other predictors fixed_.

Example: When it's very hot, ice cream sales go up and car accidents, too.
Does ice cream consumption lead to more accidents?

No. Extremely hot temperatures make people aggressive, leading to more accident-prone behavior.
Probably no big role for ice cream here.


Assessing model performance in multiple linear regression
========================================================

&nbsp;

The $R^2$ above does not work well when we have many predictors: It will only ever increase if we add new variables, however useless these may be.

Other measures like _adjusted R^2_, _AIC_ and other so called _information criteria_ take into account the number of parameters in a model.

Models with more parameters get "penalized".



Predictions from linear regression
========================================================

&nbsp;

Like for any machine learning model, predictions from least squares should always include _prediction intervals_.
If we are predicting averages, not individual examples, _confidence intervals_ will do, too.



Non-linear relationships
========================================================

Often, the relationship between $y$ and $x$ is not linear. 
It is still possible to use linear regression, using non-linear transformations (polynomials, log...) of the predictors - like in this example:

$mpg = \beta_0 + \beta_1 * horsepower + \beta_2 * horsepower^2 + \epsilon$

<figure>
<img src="3.8.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



Linear regression
========================================================

&nbsp;

- lab
- exercises


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Supervised learning (classification): logistic regression
</h1>


Who will default on their credit card?
========================================================

&nbsp;

Why we cannot just use linear regression

- on binary data, in principle we could, coding the response as 0/1
- but we would obtain some strange predictions...

<figure>
<img src="4.2.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Logistic regression - rationale
========================================================

&nbsp;

- Instead of the response itself (that is directly modeled by linear regression), logistic regression models the probability of the outcome

$P(default = Yes |  balance)$

- probabilities are between 0 and 1 - one function that does this is the _logistic_:

$p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}$

- in logistic regression, what corresponds to the regression coefficient(s) is the _log-odds_ or _logit_:

$log(\frac{p(X)}{1-p(X)}) = \beta_0 + \beta_1 X$

- this means that with a one-unit increase in X is associated an increase in the _log-odds_ of $\beta_1$


Not all errors are born equal in classification
========================================================

&nbsp;

- the simplest measure of success in classification is _accuracy_, the proportion of correct classifications 

$accuracy = \frac{number\>correctly\>classified}{number\>misclassified}$

- however, errors are either false positives or false negatives, and often it may make a big difference which it is!
- therefore, we want to look at the _confusion matrix_ that shows all 4 possibilities
- what would you think of the following confusion matrix for credit card default data?


<figure>
<img src="table_4_4.png">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Terminology
========================================================
class:smalltext

&nbsp;

action/truth                | $\mathbf{H_0}$ is true   | $\mathbf{H_1}$ is true   | total
-------------               | -------------            | -------------            | ---
__$\mathbf{H_1}$ accepted__ | $V$                      | $S$                      | $R$
__$\mathbf{H_1}$ rejected__ | $U$                      | $T$                      | $n - R$
__total__                   | $n_0$                    | $n - n_0$                | $n$


* False Positive / __Type 1 error__: $V$
* False Negative / __Type 2 error__: $T$
* __Sensitivity__ / __Recall__ / True Positive Rate: proportion of correctly detected positives, $\frac{S}{n - n_0}$
* __Specificity__ / True Negative Rate: proportion of negatives correctly identified as such, $\frac{U}{n_0}$
* __Precision__: proportion of accepted instances that are correct, $\frac{S}{R}$ (= Positive Predictive Value)
* __Accuracy__: proportion classified correctly, $\frac{S + U}{n}$

(after: http://en.wikipedia.org/wiki/Familywise_error_rate)


ROC Curves
========================================================

&nbsp;

The _area under the ROC curve_ (_AUC_) measures the overall performance of a classifier.

<figure>
<img src="4.8.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Decision boundaries
========================================================

&nbsp;

In two dimensions, it is possible to display a classifier's decision boundary.

Here are decision boundaries for two classifiers, a linear and a non-linear one (the dashed line representing the theoretical optimum):

<figure>
<img src="4.9.jpg">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Logistic regression
========================================================

&nbsp;

- lab
- exercises


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Algorithms
</h1>
<h3>
Regression and classification trees, support vector machines, k-nearest-neighbors
</h3>

Decision trees
========================================================

&nbsp;

- Decision trees partition the dataset into regions based on the predictors.
- They can be used for classification and regression.

A simple regression tree for prediction of (log) salary of baseball players:

<figure>
<img src="8.1.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Regression trees - rationale
========================================================

&nbsp;

- Regression trees divide the predictor space into regions/boxes and then predict the same value for every region/box.
- The goal is to find boxes that minimize the RSS, that is, the sum of the squared deviations of every single $y$ in a box from its "box mean" (summed over all box members and all boxes)

Here is the partitioning into regions for the baseball players example:

<figure>
<img src="8.2.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



Danger zone!
========================================================

&nbsp;

If we follow that strategy blindly, we end up _overfitting_ the dataset, as with this tree:

<figure>
<img src="8.4.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Cost complexity pruning
========================================================
class:smalltext

&nbsp;

With cost complexity pruning, we obtain the optimal tree size trading off complexity (defined as number of terminal nodes) against the RSS.

$$\sum_{m=1}^{|T|} \sum_{i: x_i \epsilon R_m} (y_i - \hat y_{R_m})^2 + \alpha|T|$$

<figure>
<img src="tree_algo.png" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Classification trees
========================================================

&nbsp;

- Classification trees are constructed analogously to regression trees
- apart from their using a different error measure: mostly one of
    - Gini index (= total variance across classes, inhomogeneity/_impurity_ of nodes)
    - cross entropy (practically equivalent in this context)



Classification tree example: Detecting heart disease
========================================================

&nbsp;

<figure>
<img src="8.6.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Trees versus linear models
========================================================

&nbsp;

- Trees are easily interpretable and optimally suited for communication
- linear models are often more robust to changes in the sample dataset, and more accurate in their predictions

In a concrete case, the shape of the true decision boundary plays an important role:

<figure>
<img src="8.7.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



Decision trees
========================================================

&nbsp;

- lab


Support vector machines
========================================================

&nbsp;

- Support vector machines have been _the hype_ out there until the event deep learning.
- They combine the _maximal margin_ principle with _kernels_ that allow for handling all kinds of non-linear data.


Separating hyperplanes
========================================================

&nbsp;

In this example, the hyperplane $1 + 2X_1 + 3X_2 = 0$ separates the blue and the red regions.

For all red points, $1 + 2X_1 + 3X_2 < 0$, while for all blue points, $1 + 2X_1 + 3X_2 > 0$.


<figure>
<img src="9.1.jpg" width="20%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

But there are many potential separating hyperplanes...
========================================================

&nbsp;

<figure>
<img src="9.2.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

Linearly separable datasets: the maximal margin classifier
========================================================

&nbsp;

The maximal margin hyperplane depends only on the _support vectors_ (shown with arrows), not on any other observations from the dataset!

<figure>
<img src="9.3.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Moving away from the linear separability condition
========================================================

&nbsp;

Most datasets are not linearly separable.

In fact, even if they are, a separating hyperplane may not always be the right choice!

<figure>
<img src="9.5.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>



The support vector classifier
========================================================

&nbsp;

The support vector classifier allows for some points to be misclassified, subject to a "misclassification budget" set by a tuning parameter.

Observations may be on the wrong side of the margin, and even on the wrong side of the hyperplane:

<figure>
<img src="9.6.jpg" width="30%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Support vectors and the bias-variance trade-off
========================================================

&nbsp;

Depending on the tuning parameter for the "misclassification budget", the resulting classifier takes a different position in the bias-variance trade-off:

- big "misclassification budget": many support vectors, low variance, potentially high bias
- small "misclassification budget": few support vectors, high variance, low bias

<figure>
<img src="9.7.jpg" width="20%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


From support vector classifier to support vector machine
========================================================

&nbsp;

No linear classifier can possibly account for this scenario...

<figure>
<img src="9.8.jpg" width="20%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


The support vector machine
========================================================

&nbsp;

Support vector machines model non-linear decision boundaries using _kernels_ (such as polynomial, radial...)

This is how SVMs with polynomial resp. radial kernel handle the above problem:

<figure>
<img src="9.9.jpg" width="20%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Support vector machine
========================================================

&nbsp;

- lab

Support vector machines
========================================================

&nbsp;

- Support vector machines have been _the hype_ out there until the event deep learning.
- They combine the _maximal margin_ principle with _kernels_ that allow for handling all kinds of non-linear data.


K-nearest neighbors (KNN)
========================================================

&nbsp;

K-nearest neighbors is a non-parametric algorithm that can be used for classification as well as regression.

A test point is classified (or assigned a prediction value) according to the k _nearest_ (by Euclidean distance, mostly) exemplars in the training set.

Note: there are no separate training and test phases!

<figure>
<img src="knn.png" width="10%">
<figcaption>Source: Wikipedia.
</figcaption>
</figure>


KNN: decision boundaries
========================================================

&nbsp;

The KNN decision boundary (and thus, the position taken in the bias-variance trade-off) depends on the parameter k.

Left: k=1, right: k=100

<figure>
<img src="2.16.jpg" width="20%">
<figcaption>Source: G. James, D. Witten,  T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>

K-nearest neighbors (KNN)
========================================================

&nbsp;

- lab


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Ensemble methods
</h1>


Many trees make up a forest
========================================================

&nbsp;

Among the most successful methods in practice are ensemble methods that _average over many runs of an algorithm_.

Where a single decision tree lacks robustness (has high variance), many trees - as in _bagging_, _boosting_ and _random forests_ - are both stable and accurate.

Bagging
========================================================

&nbsp;

- generate B bootstrapped training sets
- train a tree on every one of them and collect the predictions
- average the predictions


Random forests
========================================================

&nbsp;

- generate B bootstrapped training sets
- train a tree on every one of them
- at each step, consider just a _random subset_ of predictors for the split
- collect and average the predictions, as with bagging

As opposed to bagging, random forests build averages over more dissimilar trees, leading to a higher reduction in variance.


Boosting
========================================================

&nbsp;

- does not involve bootstrap sampling
- instead, builds a sequence of decision trees, where each new tree improves over the weaknesses of the previous ones
- i.e., a succession of "slow learners"
- each single tree often has very small depth (up to "decision stumps")

How about interpretability?
========================================================

&nbsp;


Even though with ensemble methods like random forests we lose the ease of visual inspection, we can still obtain a graph of _variable importance_, as in this example:

<figure>
<img src="8.9.jpg" width="30%">
<figcaption>Source: G. James, D. Witten, T. Hastie and R. Tibshirani, An Introduction to Statistical Learning, with applications in R (Springer, 2013)
</figcaption>
</figure>


Ensemble methods
========================================================

&nbsp;

- lab


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Model selection revisited
</h1>

<h3>Cross-validation, subset selection and regularization</h3>

Cross-validation
========================================================

&nbsp;

- lab


Subset selection
========================================================

&nbsp;

- lab


Regularization
========================================================

- TBD

Regularization
========================================================

&nbsp;

- lab

